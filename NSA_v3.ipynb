{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/taneja/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/taneja/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import igraph \n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import math\n",
    "import time\n",
    "\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and Max and Length of IDs:  10001 9912293 27770\n",
      "Length of corpus 27770\n",
      "Shape of the train TF-IDF Matrix: (27770, 25043)\n"
     ]
    }
   ],
   "source": [
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "# [['9510123', '9502114', '1'],\n",
    "#  ['9707075', '9604178', '1'],\n",
    "\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [(element[0]) for element in node_info]\n",
    "print('Min and Max and Length of IDs: ', min(IDs),max(IDs),len(IDs))\n",
    "\n",
    "# compute TFIDF vector of each paper\n",
    "corpus = [element[5] for element in node_info]\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "features_TFIDF = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print('Length of corpus',len(corpus))\n",
    "print(\"Shape of the train TF-IDF Matrix: {}\".format(features_TFIDF.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Checks On Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count and Distinct count of all Train Ids: 1231024 27770\n",
      "Count and Distinct count of all Test Ids: 65296 23402\n",
      "No.of train Ids that are not part of nodes: 0\n",
      "No.of test Ids that are not part of nodes: 0\n",
      "No.of nodes that are not part of train Ids: 0\n",
      "No.of nodes that are not part of test Ids: 4368\n"
     ]
    }
   ],
   "source": [
    "all_train_Ids = []\n",
    "for x in training_set:\n",
    "    all_train_Ids.append(x[0])\n",
    "    all_train_Ids.append(x[1])\n",
    "print('Count and Distinct count of all Train Ids:',len(all_train_Ids),len(set(all_train_Ids)))\n",
    "    \n",
    "    \n",
    "all_test_Ids = []\n",
    "for x in testing_set:\n",
    "    all_test_Ids.append(x[0])\n",
    "    all_test_Ids.append(x[1])\n",
    "print('Count and Distinct count of all Test Ids:',len(all_test_Ids),len(set(all_test_Ids)))\n",
    "    \n",
    "# Referring to slide 11 in link prediction pdf   \n",
    "print('No.of train Ids that are not part of nodes:',len(set(all_train_Ids)-set(IDs)))\n",
    "print('No.of test Ids that are not part of nodes:',len(set(all_test_Ids)-set(IDs)))\n",
    "\n",
    "print('No.of nodes that are not part of train Ids:',len(set(IDs)-set(all_train_Ids)))\n",
    "print('No.of nodes that are not part of test Ids:',len(set(IDs)-set(all_test_Ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Graphs in NetworkX and iGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iGraph nodes and edges: 27770 335130\n",
      "NetworkX nodes and edges: 27684 334690\n"
     ]
    }
   ],
   "source": [
    "edges = [(element[0],element[1]) for element in training_set if element[2]==\"1\"]\n",
    "\n",
    "# some nodes may not be connected to any other node hence the need to create nodes, not just from the edge list\n",
    "nodes = IDs\n",
    "\n",
    "## iGraph\n",
    "g = igraph.Graph(directed=True)\n",
    "g.add_vertices(nodes)\n",
    "g.add_edges(edges)\n",
    "g.summary()\n",
    "print('iGraph nodes and edges:',g.vcount(),g.ecount())\n",
    "\n",
    "# NetworkX\n",
    "G=nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "print('NetworkX nodes and edges:',G.number_of_nodes(),G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify All Graph Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n",
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_in_out_degree_prod: 800\n",
      "st_in_degree_product: 420\n",
      "st_out_degree_product: 2800\n",
      "st_in_out_degree_product: 6930\n",
      "st_common_neighbors: 0\n",
      "st_jaccard_similarity: 0.0\n",
      "st_adamic_adar: 0\n",
      "st_friendtns: 0.006\n",
      "st_katz_similarity 0\n",
      "st_count_shortest_paths_dijkstra: 7\n",
      "st_count_nodes_in_paths: 7229\n",
      "st_closeness: 0.004094187311730543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:96: RuntimeWarning: closeness centrality is not well-defined for disconnected graphs at /Users/travis/build/igraph/python-igraph/vendor/source/igraph/src/centrality.c:2856\n"
     ]
    }
   ],
   "source": [
    "source = '1001'\n",
    "target = '1002'\n",
    "\n",
    "index_source = IDs.index(source)\n",
    "index_target = IDs.index(target)\n",
    "\n",
    "# n  - Node\n",
    "# st - Source & Target\n",
    "\n",
    "def n_in_out_degree_prod(node_name):\n",
    "    prod = g.degree([node_name],type=\"in\")[0]*g.degree([node_name],type=\"out\")[0]\n",
    "    return(prod)\n",
    "print('n_in_out_degree_prod:',n_in_out_degree_prod(source))\n",
    "\n",
    "\n",
    "def st_in_degree_product(source,target):\n",
    "    prod = g.degree(source,type=\"in\") * g.degree(target,type=\"in\")\n",
    "    return(prod)\n",
    "print('st_in_degree_product:',st_in_degree_product(source,target))\n",
    "\n",
    "\n",
    "def st_out_degree_product(source,target):\n",
    "    prod = g.degree(source,type=\"out\") * g.degree(target,type=\"out\")\n",
    "    return(prod)\n",
    "print('st_out_degree_product:',st_out_degree_product(source,target))\n",
    "\n",
    "def st_in_out_degree_product(source,target):\n",
    "    prod = g.degree(source) * g.degree(target)\n",
    "    return(prod)\n",
    "print('st_in_out_degree_product:',st_in_out_degree_product(source,target))\n",
    "\n",
    "def st_common_neighbors(source,target):\n",
    "    count = len(set(g.neighbors(source)).intersection(g.neighbors(target)))\n",
    "    return(count)\n",
    "print('st_common_neighbors:',st_common_neighbors(source,target))\n",
    "\n",
    "def st_jaccard_similarity(source,target):\n",
    "    nr = len(set(g.neighbors(source)).intersection(set(g.neighbors(target))))\n",
    "    dr = float(len(set(g.neighbors(source)).union(set(g.neighbors(target)))))\n",
    "    return(nr/dr)\n",
    "print('st_jaccard_similarity:',st_jaccard_similarity(source,target))\n",
    "\n",
    "def st_adamic_adar(source,target):\n",
    "    ans = sum([1.0/math.log(g.degree(v)) for v in set(g.neighbors(source)).intersection(set(g.neighbors(target)))])\n",
    "    return(ans)\n",
    "print('st_adamic_adar:',st_adamic_adar(source,target))\n",
    "\n",
    "def st_friendtns(source,target):\n",
    "    ans = round((1.0/(g.degree(source) + g.degree(target) - 1.0)),3)\n",
    "    return(ans)\n",
    "print('st_friendtns:',st_friendtns(source,target))\n",
    " \n",
    "def katz(g):\n",
    "    katzDict = {}  # build a special dict that is like an adjacency list\n",
    "    adjList = g.get_adjlist()\n",
    "\n",
    "    for i, l in enumerate(adjList):\n",
    "        katzDict[i] = l\n",
    "    return katzDict\n",
    "        \n",
    "def st_katz_similarity(katzDict,i,j):\n",
    "    l = 1\n",
    "    maxl = 5\n",
    "    neighbors = katzDict[i]\n",
    "    score = 0\n",
    "    beta = 0.005 \n",
    "\n",
    "    while l <= maxl:\n",
    "        numberOfPaths = neighbors.count(j)\n",
    "        if numberOfPaths > 0:\n",
    "            score += (beta**l)*numberOfPaths\n",
    "\n",
    "        neighborsForNextLoop = []\n",
    "        for k in neighbors:\n",
    "            neighborsForNextLoop += katzDict[k]\n",
    "        neighbors = neighborsForNextLoop\n",
    "        l += 1\n",
    "\n",
    "    return score\n",
    "\n",
    "katzDict = katz(g)\n",
    "print('st_katz_similarity',st_katz_similarity(katzDict,index_source,index_target))\n",
    "\n",
    "def st_count_shortest_paths_dijkstra(source,target):\n",
    "    ans = g.shortest_paths_dijkstra(source, target)[0][0]\n",
    "    return(ans)\n",
    "print('st_count_shortest_paths_dijkstra:',st_count_shortest_paths_dijkstra(source,target))\n",
    "\n",
    "def st_count_nodes_in_paths(source,target):\n",
    "    s=set(g.subcomponent(source, mode=\"out\"))\n",
    "    t=set(g.subcomponent(target, mode=\"in\"))\n",
    "    return(len(s.intersection(t)))\n",
    "print('st_count_nodes_in_paths:',st_count_nodes_in_paths(source,target))\n",
    "\n",
    "def st_closeness(source,target):\n",
    "    ans = g.closeness(vertices=source)+g.closeness(vertices=target)\n",
    "    return(ans)\n",
    "print('st_closeness:',st_closeness(source,target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Features Of Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n",
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n",
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:96: RuntimeWarning: closeness centrality is not well-defined for disconnected graphs at /Users/travis/build/igraph/python-igraph/vendor/source/igraph/src/centrality.c:2856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for 1 rows is 0 mins\n",
      "Training time for 1001 rows is 1 mins\n",
      "Training time for 2001 rows is 2 mins\n",
      "Training time for 3001 rows is 3 mins\n",
      "Training time for 4001 rows is 4 mins\n",
      "Training time for 5001 rows is 5 mins\n",
      "Training time for 6001 rows is 6 mins\n",
      "Training time for 7001 rows is 8 mins\n",
      "Training time for 8001 rows is 9 mins\n",
      "Training time for 9001 rows is 10 mins\n",
      "Training time for 10001 rows is 11 mins\n",
      "Training time for 11001 rows is 12 mins\n",
      "Training time for 12001 rows is 13 mins\n",
      "Training time for 13001 rows is 15 mins\n",
      "Training time for 14001 rows is 16 mins\n",
      "Training time for 15001 rows is 17 mins\n",
      "Training time for 16001 rows is 18 mins\n",
      "Training time for 17001 rows is 19 mins\n",
      "Training time for 18001 rows is 20 mins\n",
      "Training time for 19001 rows is 21 mins\n",
      "Training time for 20001 rows is 23 mins\n",
      "Training time for 21001 rows is 24 mins\n"
     ]
    }
   ],
   "source": [
    "# randomly select 5% of training set\n",
    "to_keep = random.sample(range(len(training_set)), k=int(round(len(training_set)*0.05)))\n",
    "training_set_reduced = [training_set[i] for i in to_keep]\n",
    "# training_set_reduced = training_set\n",
    "\n",
    "overlap_title = []                         # number of overlapping words in title\n",
    "temp_diff = []                             # temporal distance between the papers\n",
    "comm_auth = []                             # number of common authors\n",
    "ln_in_out_degree_prod_source = []          # \n",
    "ln_in_out_degree_prod_target = []          # \n",
    "lst_in_degree_product = []                 # \n",
    "lst_out_degree_product = []                # \n",
    "lst_in_out_degree_product = []             # \n",
    "lst_common_neighbors = []                  # \n",
    "lst_jaccard_similarity = []                # \n",
    "lst_adamic_adar = []                       # \n",
    "lst_friendtns = []                         # \n",
    "lst_katz_similarity = []                   # \n",
    "lst_count_shortest_paths_dijkstra = []     # \n",
    "lst_count_nodes_in_paths = []              # \n",
    "lst_closeness = []                         # \n",
    "\n",
    "counter = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(len(training_set_reduced)):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    # Metadata Features\n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "   \n",
    "    # Graph Features\n",
    "    ln_in_out_degree_prod_source.append(n_in_out_degree_prod(source))\n",
    "    ln_in_out_degree_prod_target.append(n_in_out_degree_prod(target))\n",
    "    lst_in_degree_product.append(st_in_degree_product(source,target))\n",
    "    lst_out_degree_product.append(st_out_degree_product(source,target))\n",
    "    lst_in_out_degree_product.append(st_in_out_degree_product(source,target))\n",
    "    lst_common_neighbors.append(st_common_neighbors(source,target))\n",
    "    lst_jaccard_similarity.append(st_jaccard_similarity(source,target))\n",
    "    lst_adamic_adar.append(st_adamic_adar(source,target))\n",
    "#     lst_friendtns.append(st_friendtns(source,target))\n",
    "#     lst_katz_similarity.append(st_katz_similarity(katzDict,index_source,index_target))\n",
    "    lst_count_shortest_paths_dijkstra.append(st_count_shortest_paths_dijkstra(source,target))\n",
    "    lst_count_nodes_in_paths.append(st_count_nodes_in_paths(source,target))\n",
    "    lst_closeness.append(st_closeness(source,target))\n",
    "\n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "#         print(counter, \"training examples processsed\")\n",
    "            print(f'Training time for {counter} rows is {int((time.time()-start_time)/60)} mins')\n",
    "\n",
    "# convert list of lists into array\n",
    "training_features = np.array([\n",
    "                            overlap_title, \n",
    "                            temp_diff, \n",
    "                            comm_auth,\n",
    "                            ln_in_out_degree_prod_source,\n",
    "                            ln_in_out_degree_prod_target,\n",
    "                            lst_in_degree_product,\n",
    "                            lst_out_degree_product,\n",
    "                            lst_in_out_degree_product,\n",
    "                            lst_common_neighbors,\n",
    "                            lst_jaccard_similarity,\n",
    "                            lst_adamic_adar,\n",
    "#                             lst_friendtns,\n",
    "#                             lst_katz_similarity,\n",
    "                            lst_count_shortest_paths_dijkstra,\n",
    "                            lst_count_nodes_in_paths,\n",
    "                            lst_closeness]).T\n",
    "\n",
    "print('Total Train time in minutes',int((time.time()-start_time)/60))\n",
    "\n",
    "# scale\n",
    "training_features = preprocessing.scale(training_features)\n",
    "\n",
    "# convert labels into integers then into column array\n",
    "labels = [int(element[2]) for element in training_set_reduced]\n",
    "labels = list(labels)\n",
    "labels_array = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Features Of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overlap_title = []                         # number of overlapping words in title\n",
    "temp_diff = []                             # temporal distance between the papers\n",
    "comm_auth = []                             # number of common authors\n",
    "ln_in_out_degree_prod_source = []          # \n",
    "ln_in_out_degree_prod_target = []          # \n",
    "lst_in_degree_product = []                 # \n",
    "lst_out_degree_product = []                # \n",
    "lst_in_out_degree_product = []             # \n",
    "lst_common_neighbors = []                  # \n",
    "lst_jaccard_similarity = []                # \n",
    "lst_adamic_adar = []                       # \n",
    "lst_friendtns = []                         # \n",
    "lst_katz_similarity = []                   # \n",
    "lst_count_shortest_paths_dijkstra = []     # \n",
    "lst_count_nodes_in_paths = []              # \n",
    "lst_closeness = []                         # \n",
    "\n",
    "counter = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "\n",
    "    # Metadata Features    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title_test.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff_test.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth_test.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "\n",
    "    # Graph Features\n",
    "    ln_in_out_degree_prod_source.append(n_in_out_degree_prod(source))\n",
    "    ln_in_out_degree_prod_target.append(n_in_out_degree_prod(target))\n",
    "    lst_in_degree_product.append(st_in_degree_product(source,target))\n",
    "    lst_out_degree_product.append(st_out_degree_product(source,target))\n",
    "    lst_in_out_degree_product.append(st_in_out_degree_product(source,target))\n",
    "    lst_common_neighbors.append(st_common_neighbors(source,target))\n",
    "    lst_jaccard_similarity.append(st_jaccard_similarity(source,target))\n",
    "    lst_adamic_adar.append(st_adamic_adar(source,target))\n",
    "#     lst_friendtns.append(st_friendtns(source,target))\n",
    "#     lst_katz_similarity.append(st_katz_similarity(katzDict,index_source,index_target))\n",
    "    lst_count_shortest_paths_dijkstra.append(st_count_shortest_paths_dijkstra(source,target))\n",
    "    lst_count_nodes_in_paths.append(st_count_nodes_in_paths(source,target))\n",
    "    lst_closeness.append(st_closeness(source,target))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "#         print(counter, \"testing examples processsed\")\n",
    "        print(f'Testing time for {counter} rows is {int((time.time()-start_time)/60)} mins')\n",
    "        \n",
    "# convert list of lists into array\n",
    "testing_features = np.array([\n",
    "                            overlap_title, \n",
    "                            temp_diff, \n",
    "                            comm_auth,\n",
    "                            ln_in_out_degree_prod_source,\n",
    "                            ln_in_out_degree_prod_target,\n",
    "                            lst_in_degree_product,\n",
    "                            lst_out_degree_product,\n",
    "                            lst_in_out_degree_product,\n",
    "                            lst_common_neighbors,\n",
    "                            lst_jaccard_similarity,\n",
    "                            lst_adamic_adar,\n",
    "#                             lst_friendtns,\n",
    "#                             lst_katz_similarity,\n",
    "                            lst_count_shortest_paths_dijkstra,\n",
    "                            lst_count_nodes_in_paths,\n",
    "                            lst_closeness]).T\n",
    "\n",
    "print('Total Train time in minutes',int((time.time()-start_time)/60))\n",
    "\n",
    "# scale\n",
    "testing_features = preprocessing.scale(testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(training_features, labels_array)\n",
    "predictions_SVM = list(classifier.predict(testing_features))\n",
    "\n",
    "df_out = pd.DataFrame({'id':range(len(testing_set)),'category':predictions_SVM})\n",
    "print(df_out.head())\n",
    "df_out.to_csv('sub4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Experiments__\n",
    "- Sub1: 100% data | 0.53548 | wrong code in testing part <br>\n",
    "_Corrected code in the testing part_\n",
    "- Sub2: 5% data   | 0.71251 |\n",
    "- Sub3: 100% data | 0.71251 | doubtful that its the same as Sub2 <br>\n",
    "_Until now only 3 features are used_\n",
    "\n",
    "***\n",
    "\n",
    "__Next Steps__\n",
    "- Add tf-idf cosine similarity between source and target as extra feature\n",
    "- Check if Gensim is sensible to do instead of tf-idf\n",
    "- Apply keras neural network with all features including tf-idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
