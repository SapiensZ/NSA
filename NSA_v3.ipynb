{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import igraph \n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import math\n",
    "import time\n",
    "from numpy import inf\n",
    "\n",
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and Max and Length of IDs:  10001 9912293 27770\n",
      "Length of corpus 27770\n",
      "Shape of the train TF-IDF Matrix: (27770, 25043)\n"
     ]
    }
   ],
   "source": [
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "# [['9510123', '9502114', '1'],\n",
    "#  ['9707075', '9604178', '1'],\n",
    "\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [(element[0]) for element in node_info]\n",
    "print('Min and Max and Length of IDs: ', min(IDs),max(IDs),len(IDs))\n",
    "\n",
    "# compute TFIDF vector of each paper\n",
    "corpus = [element[5] for element in node_info]\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "features_TFIDF = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print('Length of corpus',len(corpus))\n",
    "print(\"Shape of the train TF-IDF Matrix: {}\".format(features_TFIDF.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Checks On Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count and Distinct count of all Train Ids: 1231024 27770\n",
      "Count and Distinct count of all Test Ids: 65296 23402\n",
      "No.of train Ids that are not part of nodes: 0\n",
      "No.of test Ids that are not part of nodes: 0\n",
      "No.of nodes that are not part of train Ids: 0\n",
      "No.of nodes that are not part of test Ids: 4368\n"
     ]
    }
   ],
   "source": [
    "all_train_Ids = []\n",
    "for x in training_set:\n",
    "    all_train_Ids.append(x[0])\n",
    "    all_train_Ids.append(x[1])\n",
    "print('Count and Distinct count of all Train Ids:',len(all_train_Ids),len(set(all_train_Ids)))\n",
    "    \n",
    "    \n",
    "all_test_Ids = []\n",
    "for x in testing_set:\n",
    "    all_test_Ids.append(x[0])\n",
    "    all_test_Ids.append(x[1])\n",
    "print('Count and Distinct count of all Test Ids:',len(all_test_Ids),len(set(all_test_Ids)))\n",
    "    \n",
    "# Referring to slide 11 in link prediction pdf   \n",
    "print('No.of train Ids that are not part of nodes:',len(set(all_train_Ids)-set(IDs)))\n",
    "print('No.of test Ids that are not part of nodes:',len(set(all_test_Ids)-set(IDs)))\n",
    "\n",
    "print('No.of nodes that are not part of train Ids:',len(set(IDs)-set(all_train_Ids)))\n",
    "print('No.of nodes that are not part of test Ids:',len(set(IDs)-set(all_test_Ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Graphs in NetworkX and iGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iGraph nodes and edges: 27770 335130\n",
      "NetworkX nodes and edges: 27684 334690\n"
     ]
    }
   ],
   "source": [
    "edges = [(element[0],element[1]) for element in training_set if element[2]==\"1\"]\n",
    "\n",
    "# some nodes may not be connected to any other node hence the need to create nodes, not just from the edge list\n",
    "nodes = IDs\n",
    "\n",
    "## iGraph\n",
    "g = igraph.Graph(directed=True)\n",
    "g.add_vertices(nodes)\n",
    "g.add_edges(edges)\n",
    "g.summary()\n",
    "print('iGraph nodes and edges:',g.vcount(),g.ecount())\n",
    "\n",
    "# NetworkX\n",
    "G=nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "print('NetworkX nodes and edges:',G.number_of_nodes(),G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify All Graph Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n",
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_in_out_degree_prod: 800\n",
      "st_in_degree_product: 420\n",
      "st_out_degree_product: 2800\n",
      "st_in_out_degree_product: 6930\n",
      "st_common_neighbors: 0\n",
      "st_jaccard_similarity: 0.0\n",
      "st_adamic_adar: 0\n",
      "st_friendtns: 0.006\n",
      "st_katz_similarity 0\n",
      "st_count_shortest_paths_dijkstra: 7\n",
      "st_count_nodes_in_paths: 7229\n",
      "st_closeness: 0.004094187311730543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:101: RuntimeWarning: closeness centrality is not well-defined for disconnected graphs at /Users/travis/build/igraph/python-igraph/vendor/source/igraph/src/centrality.c:2856\n"
     ]
    }
   ],
   "source": [
    "source = '1001'\n",
    "target = '1002'\n",
    "\n",
    "index_source = IDs.index(source)\n",
    "index_target = IDs.index(target)\n",
    "\n",
    "# n  - Node\n",
    "# st - Source & Target\n",
    "\n",
    "def n_in_out_degree_prod(node_name):\n",
    "    prod = g.degree([node_name],type=\"in\")[0]*g.degree([node_name],type=\"out\")[0]\n",
    "    return(prod)\n",
    "print('n_in_out_degree_prod:',n_in_out_degree_prod(source))\n",
    "\n",
    "\n",
    "def st_in_degree_product(source,target):\n",
    "    prod = g.degree(source,type=\"in\") * g.degree(target,type=\"in\")\n",
    "    return(prod)\n",
    "print('st_in_degree_product:',st_in_degree_product(source,target))\n",
    "\n",
    "\n",
    "def st_out_degree_product(source,target):\n",
    "    prod = g.degree(source,type=\"out\") * g.degree(target,type=\"out\")\n",
    "    return(prod)\n",
    "print('st_out_degree_product:',st_out_degree_product(source,target))\n",
    "\n",
    "def st_in_out_degree_product(source,target):\n",
    "    prod = g.degree(source) * g.degree(target)\n",
    "    return(prod)\n",
    "print('st_in_out_degree_product:',st_in_out_degree_product(source,target))\n",
    "\n",
    "def st_common_neighbors(source,target):\n",
    "    count = len(set(g.neighbors(source)).intersection(g.neighbors(target)))\n",
    "    return(count)\n",
    "print('st_common_neighbors:',st_common_neighbors(source,target))\n",
    "\n",
    "def st_jaccard_similarity(source,target):\n",
    "    nr = len(set(g.neighbors(source)).intersection(set(g.neighbors(target))))\n",
    "    dr = float(len(set(g.neighbors(source)).union(set(g.neighbors(target)))))\n",
    "    \n",
    "    if dr==0:\n",
    "        ans=0\n",
    "    else:\n",
    "        ans = nr/dr\n",
    "    return(ans)\n",
    "print('st_jaccard_similarity:',st_jaccard_similarity(source,target))\n",
    "\n",
    "def st_adamic_adar(source,target):\n",
    "    ans = sum([1.0/math.log(g.degree(v)) for v in set(g.neighbors(source)).intersection(set(g.neighbors(target)))])\n",
    "    return(ans)\n",
    "print('st_adamic_adar:',st_adamic_adar(source,target))\n",
    "\n",
    "def st_friendtns(source,target):\n",
    "    ans = round((1.0/(g.degree(source) + g.degree(target) - 1.0)),3)\n",
    "    return(ans)\n",
    "print('st_friendtns:',st_friendtns(source,target))\n",
    " \n",
    "def katz(g):\n",
    "    katzDict = {}  # build a special dict that is like an adjacency list\n",
    "    adjList = g.get_adjlist()\n",
    "\n",
    "    for i, l in enumerate(adjList):\n",
    "        katzDict[i] = l\n",
    "    return katzDict\n",
    "        \n",
    "def st_katz_similarity(katzDict,i,j):\n",
    "    l = 1\n",
    "    maxl = 5\n",
    "    neighbors = katzDict[i]\n",
    "    score = 0\n",
    "    beta = 0.005 \n",
    "\n",
    "    while l <= maxl:\n",
    "        numberOfPaths = neighbors.count(j)\n",
    "        if numberOfPaths > 0:\n",
    "            score += (beta**l)*numberOfPaths\n",
    "\n",
    "        neighborsForNextLoop = []\n",
    "        for k in neighbors:\n",
    "            neighborsForNextLoop += katzDict[k]\n",
    "        neighbors = neighborsForNextLoop\n",
    "        l += 1\n",
    "\n",
    "    return score\n",
    "\n",
    "katzDict = katz(g)\n",
    "print('st_katz_similarity',st_katz_similarity(katzDict,index_source,index_target))\n",
    "\n",
    "def st_count_shortest_paths_dijkstra(source,target):\n",
    "    ans = g.shortest_paths_dijkstra(source, target)[0][0]\n",
    "    return(ans)\n",
    "print('st_count_shortest_paths_dijkstra:',st_count_shortest_paths_dijkstra(source,target))\n",
    "\n",
    "def st_count_nodes_in_paths(source,target):\n",
    "    s=set(g.subcomponent(source, mode=\"out\"))\n",
    "    t=set(g.subcomponent(target, mode=\"in\"))\n",
    "    return(len(s.intersection(t)))\n",
    "print('st_count_nodes_in_paths:',st_count_nodes_in_paths(source,target))\n",
    "\n",
    "def st_closeness(source,target):\n",
    "    ans = g.closeness(vertices=source)+g.closeness(vertices=target)\n",
    "    return(ans)\n",
    "print('st_closeness:',st_closeness(source,target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Features Of Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n",
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n",
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:96: RuntimeWarning: closeness centrality is not well-defined for disconnected graphs at /Users/travis/build/igraph/python-igraph/vendor/source/igraph/src/centrality.c:2856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for 1 rows is 0 mins\n",
      "Training time for 1001 rows is 1 mins\n",
      "Training time for 2001 rows is 2 mins\n",
      "Training time for 3001 rows is 3 mins\n",
      "Training time for 4001 rows is 4 mins\n",
      "Training time for 5001 rows is 5 mins\n",
      "Training time for 6001 rows is 6 mins\n",
      "Training time for 7001 rows is 8 mins\n",
      "Training time for 8001 rows is 9 mins\n",
      "Training time for 9001 rows is 10 mins\n",
      "Training time for 10001 rows is 11 mins\n",
      "Training time for 11001 rows is 12 mins\n",
      "Training time for 12001 rows is 13 mins\n",
      "Training time for 13001 rows is 15 mins\n",
      "Training time for 14001 rows is 16 mins\n",
      "Training time for 15001 rows is 17 mins\n",
      "Training time for 16001 rows is 18 mins\n",
      "Training time for 17001 rows is 19 mins\n",
      "Training time for 18001 rows is 20 mins\n",
      "Training time for 19001 rows is 21 mins\n",
      "Training time for 20001 rows is 23 mins\n",
      "Training time for 21001 rows is 24 mins\n",
      "Training time for 22001 rows is 25 mins\n",
      "Training time for 23001 rows is 26 mins\n",
      "Training time for 24001 rows is 27 mins\n",
      "Training time for 25001 rows is 28 mins\n",
      "Training time for 26001 rows is 30 mins\n",
      "Training time for 27001 rows is 31 mins\n",
      "Training time for 28001 rows is 32 mins\n",
      "Training time for 29001 rows is 33 mins\n",
      "Training time for 30001 rows is 34 mins\n",
      "Total Train time in minutes 35\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b75d658c3fa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mtraining_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# convert labels into integers then into column array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mscale\u001b[0;34m(X, axis, with_mean, with_std, copy)\u001b[0m\n\u001b[1;32m    139\u001b[0m     X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,\n\u001b[1;32m    140\u001b[0m                     \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'the scale function'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# randomly select 5% of training set\n",
    "to_keep = random.sample(range(len(training_set)), k=int(round(len(training_set)*0.05)))\n",
    "training_set_reduced = [training_set[i] for i in to_keep]\n",
    "# training_set_reduced = training_set\n",
    "\n",
    "overlap_title = []                      # number of overlapping words in title\n",
    "temp_diff = []                          # temporal distance between the papers\n",
    "comm_auth = []                          # number of common authors\n",
    "ln_in_out_degree_prod_source = []       # product of in-degrees and out-degrees of source\n",
    "ln_in_out_degree_prod_target = []       # product of in-degrees and out-degrees of target\n",
    "lst_in_degree_product = []              # product of in-degrees of source and target \n",
    "lst_out_degree_product = []             # product of out-degrees of source and target\n",
    "lst_in_out_degree_product = []          # product of in-degrees of x and out-degrees of y (preferential attachments)\n",
    "lst_common_neighbors = []               # number of common nodes\n",
    "lst_jaccard_similarity = []             # probability that both source and target have common neighbors\n",
    "lst_adamic_adar = []                    # large weights to common nodes of x&y, which themselves have few neighbors\n",
    "lst_friendtns = []                      # \n",
    "lst_katz_similarity = []                # measures influence based on all paths between a pair of nodes\n",
    "lst_count_shortest_paths_dijkstra = []  # \n",
    "lst_count_nodes_in_paths = []           # \n",
    "lst_closeness = []                      # \n",
    "\n",
    "counter = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(len(training_set_reduced)):\n",
    "    source = training_set_reduced[i][0]\n",
    "    target = training_set_reduced[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    \n",
    "    # Metadata Features\n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "   \n",
    "    # Graph Features\n",
    "    ln_in_out_degree_prod_source.append(n_in_out_degree_prod(source))\n",
    "    ln_in_out_degree_prod_target.append(n_in_out_degree_prod(target))\n",
    "    lst_in_degree_product.append(st_in_degree_product(source,target))\n",
    "    lst_out_degree_product.append(st_out_degree_product(source,target))\n",
    "    lst_in_out_degree_product.append(st_in_out_degree_product(source,target))\n",
    "    lst_common_neighbors.append(st_common_neighbors(source,target))\n",
    "    lst_jaccard_similarity.append(st_jaccard_similarity(source,target))\n",
    "    lst_adamic_adar.append(st_adamic_adar(source,target))\n",
    "#     lst_friendtns.append(st_friendtns(source,target))\n",
    "#     lst_katz_similarity.append(st_katz_similarity(katzDict,index_source,index_target))\n",
    "    lst_count_shortest_paths_dijkstra.append(st_count_shortest_paths_dijkstra(source,target))\n",
    "    lst_count_nodes_in_paths.append(st_count_nodes_in_paths(source,target))\n",
    "    lst_closeness.append(st_closeness(source,target))\n",
    "\n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "#         print(counter, \"training examples processsed\")\n",
    "            print(f'Training time for {counter} rows is {int((time.time()-start_time)/60)} mins')\n",
    "\n",
    "# convert list of lists into array\n",
    "training_features = np.array([\n",
    "                            overlap_title, \n",
    "                            temp_diff, \n",
    "                            comm_auth,\n",
    "                            ln_in_out_degree_prod_source,\n",
    "                            ln_in_out_degree_prod_target,\n",
    "                            lst_in_degree_product,\n",
    "                            lst_out_degree_product,\n",
    "                            lst_in_out_degree_product,\n",
    "                            lst_common_neighbors,\n",
    "                            lst_jaccard_similarity,\n",
    "                            lst_adamic_adar,\n",
    "#                             lst_friendtns,\n",
    "#                             lst_katz_similarity,\n",
    "                            lst_count_shortest_paths_dijkstra,\n",
    "                            lst_count_nodes_in_paths,\n",
    "                            lst_closeness]).T\n",
    "\n",
    "print('Total Train time in minutes',int((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dijkstra edits: replace infinites with zeros\n",
    "training_features[training_features == inf] = 0\n",
    "\n",
    "# scale\n",
    "training_features = preprocessing.scale(training_features)\n",
    "\n",
    "# convert labels into integers then into column array\n",
    "labels = [int(element[2]) for element in training_set_reduced]\n",
    "labels = list(labels)\n",
    "labels_array = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Features Of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n",
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: type=... keyword argument is deprecated since igraph 0.6, use mode=... instead\n",
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:101: RuntimeWarning: closeness centrality is not well-defined for disconnected graphs at /Users/travis/build/igraph/python-igraph/vendor/source/igraph/src/centrality.c:2856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time for 1 rows is 0 mins\n",
      "Testing time for 1001 rows is 1 mins\n",
      "Testing time for 2001 rows is 2 mins\n",
      "Testing time for 3001 rows is 3 mins\n",
      "Testing time for 4001 rows is 4 mins\n",
      "Testing time for 5001 rows is 5 mins\n",
      "Testing time for 6001 rows is 6 mins\n",
      "Testing time for 7001 rows is 8 mins\n",
      "Testing time for 8001 rows is 9 mins\n",
      "Testing time for 9001 rows is 10 mins\n",
      "Testing time for 10001 rows is 11 mins\n",
      "Testing time for 11001 rows is 12 mins\n",
      "Testing time for 12001 rows is 13 mins\n",
      "Testing time for 13001 rows is 14 mins\n",
      "Testing time for 14001 rows is 16 mins\n",
      "Testing time for 15001 rows is 17 mins\n",
      "Testing time for 16001 rows is 18 mins\n",
      "Testing time for 17001 rows is 19 mins\n",
      "Testing time for 18001 rows is 20 mins\n",
      "Testing time for 19001 rows is 21 mins\n",
      "Testing time for 20001 rows is 23 mins\n",
      "Testing time for 21001 rows is 24 mins\n",
      "Testing time for 22001 rows is 25 mins\n",
      "Testing time for 23001 rows is 26 mins\n",
      "Testing time for 24001 rows is 27 mins\n",
      "Testing time for 25001 rows is 28 mins\n",
      "Testing time for 26001 rows is 29 mins\n",
      "Testing time for 27001 rows is 31 mins\n",
      "Testing time for 28001 rows is 32 mins\n",
      "Testing time for 29001 rows is 33 mins\n",
      "Testing time for 30001 rows is 34 mins\n",
      "Testing time for 31001 rows is 35 mins\n",
      "Testing time for 32001 rows is 36 mins\n",
      "Total Train time in minutes 37\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-eb0bdab11a19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mtesting_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Anaconda3/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mscale\u001b[0;34m(X, axis, with_mean, with_std, copy)\u001b[0m\n\u001b[1;32m    139\u001b[0m     X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,\n\u001b[1;32m    140\u001b[0m                     \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'the scale function'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "overlap_title = []                         # number of overlapping words in title\n",
    "temp_diff = []                             # temporal distance between the papers\n",
    "comm_auth = []                             # number of common authors\n",
    "ln_in_out_degree_prod_source = []          # \n",
    "ln_in_out_degree_prod_target = []          # \n",
    "lst_in_degree_product = []                 # \n",
    "lst_out_degree_product = []                # \n",
    "lst_in_out_degree_product = []             # \n",
    "lst_common_neighbors = []                  # \n",
    "lst_jaccard_similarity = []                # \n",
    "lst_adamic_adar = []                       # \n",
    "lst_friendtns = []                         # \n",
    "lst_katz_similarity = []                   # \n",
    "lst_count_shortest_paths_dijkstra = []     # \n",
    "lst_count_nodes_in_paths = []              # \n",
    "lst_closeness = []                         # \n",
    "\n",
    "counter = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "\n",
    "    # Metadata Features    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "\n",
    "    # Graph Features\n",
    "    ln_in_out_degree_prod_source.append(n_in_out_degree_prod(source))\n",
    "    ln_in_out_degree_prod_target.append(n_in_out_degree_prod(target))\n",
    "    lst_in_degree_product.append(st_in_degree_product(source,target))\n",
    "    lst_out_degree_product.append(st_out_degree_product(source,target))\n",
    "    lst_in_out_degree_product.append(st_in_out_degree_product(source,target))\n",
    "    lst_common_neighbors.append(st_common_neighbors(source,target))\n",
    "    lst_jaccard_similarity.append(st_jaccard_similarity(source,target))\n",
    "    lst_adamic_adar.append(st_adamic_adar(source,target))\n",
    "#     lst_friendtns.append(st_friendtns(source,target))\n",
    "#     lst_katz_similarity.append(st_katz_similarity(katzDict,index_source,index_target))\n",
    "    lst_count_shortest_paths_dijkstra.append(st_count_shortest_paths_dijkstra(source,target))\n",
    "    lst_count_nodes_in_paths.append(st_count_nodes_in_paths(source,target))\n",
    "    lst_closeness.append(st_closeness(source,target))\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 1000 == True:\n",
    "#         print(counter, \"testing examples processsed\")\n",
    "        print(f'Testing time for {counter} rows is {int((time.time()-start_time)/60)} mins')\n",
    "        \n",
    "# convert list of lists into array\n",
    "testing_features = np.array([\n",
    "                            overlap_title, \n",
    "                            temp_diff, \n",
    "                            comm_auth,\n",
    "                            ln_in_out_degree_prod_source,\n",
    "                            ln_in_out_degree_prod_target,\n",
    "                            lst_in_degree_product,\n",
    "                            lst_out_degree_product,\n",
    "                            lst_in_out_degree_product,\n",
    "                            lst_common_neighbors,\n",
    "                            lst_jaccard_similarity,\n",
    "                            lst_adamic_adar,\n",
    "#                             lst_friendtns,\n",
    "#                             lst_katz_similarity,\n",
    "                            lst_count_shortest_paths_dijkstra,\n",
    "                            lst_count_nodes_in_paths,\n",
    "                            lst_closeness]).T\n",
    "\n",
    "print('Total Train time in minutes',int((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dijkstra edits: replace infinites with zeros\n",
    "testing_features[testing_features == inf] = 0\n",
    "\n",
    "# scale\n",
    "testing_features = preprocessing.scale(testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  category\n",
      "0   0         0\n",
      "1   1         1\n",
      "2   2         1\n",
      "3   3         1\n",
      "4   4         0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taneja/Anaconda3/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(training_features, labels_array)\n",
    "predictions_SVM = list(classifier.predict(testing_features))\n",
    "\n",
    "df_out = pd.DataFrame({'id':range(len(testing_set)),'category':predictions_SVM})\n",
    "print(df_out.head())\n",
    "df_out.to_csv('sub4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Experiments__\n",
    "- Sub1: 100% data | 0.53548 | wrong code in testing part <br>\n",
    "_Corrected code in the testing part_\n",
    "- Sub2: 5% data   | 0.71251 |\n",
    "- Sub3: 100% data | 0.71251 | doubtful that its the same as Sub2 <br>\n",
    "_Until now only 3 features are used_\n",
    "\n",
    "***\n",
    "\n",
    "__Next Steps__\n",
    "- Add tf-idf cosine similarity between source and target as extra feature\n",
    "- Check if Gensim is sensible to do instead of tf-idf\n",
    "- Apply keras neural network with all features including tf-idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
